{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making an array with ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "1\n",
      "[[1 1 1 1 1]\n",
      " [1 1 1 1 1]\n",
      " [1 1 0 1 1]\n",
      " [1 1 1 1 1]\n",
      " [1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "w = numpy.ones([5,5],dtype=numpy.int8)\n",
    "\n",
    "h=int(w.shape[0]/2)\n",
    "r=int(w.shape[1]/2)\n",
    "\n",
    "print(h,r)\n",
    "print(w[h,r])\n",
    "\n",
    "w[h][r]=0\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "1\n",
      "[[1 1 1 1 1]\n",
      " [1 1 1 1 1]\n",
      " [1 1 0 1 1]\n",
      " [1 1 1 1 1]\n",
      " [1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Explained above code\n",
    "import numpy\n",
    "\n",
    "w = numpy.ones([5,5], dtype=numpy.int8)  # Create a 5x5 array of ones (int8 type)\n",
    "\n",
    "h = int(w.shape[0] / 2)  # h = 2 (center row)\n",
    "r = int(w.shape[1] / 2)  # r = 2 (center column)\n",
    "\n",
    "print(h, r)              # Should print: 2 2\n",
    "print(w[h, r])           # Should print: 1 (center value)\n",
    "\n",
    "w[h][r] = 0              # Set center value to 0\n",
    "print(w)                 # Should print the matrix with center value set to 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicing forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "def binary_cross_entropy_loss(y_true, y_pred):\n",
    "    # Ensure predicted values are in the range [0, 1]\n",
    "    epsilon=1e-7\n",
    "    y_pred = torch.clamp(y_pred, epsilon, 1 - epsilon)  # Avoid log(0) issues\n",
    "    loss = -(y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "# Initaial Parameters\n",
    "x = torch.tensor(5.5)\n",
    "y = torch.tensor(0)\n",
    "\n",
    "# Parameters with gradients\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 5.504086971282959\n",
      "2 5.194328308105469\n",
      "3 4.885583877563477\n",
      "4 4.578197479248047\n",
      "5 4.272622585296631\n",
      "6 3.969496965408325\n",
      "7 3.669649600982666\n",
      "8 3.3741464614868164\n",
      "9 3.084341526031494\n",
      "10 2.8019165992736816\n",
      "11 2.5288779735565186\n",
      "12 2.267523765563965\n",
      "13 2.02032470703125\n",
      "14 1.7897261381149292\n",
      "15 1.577878475189209\n",
      "16 1.3863532543182373\n",
      "17 1.2159124612808228\n",
      "18 1.0664112567901611\n",
      "19 0.9368649125099182\n",
      "20 0.8256462812423706\n",
      "21 0.7307476997375488\n",
      "22 0.6500300168991089\n",
      "23 0.5814123749732971\n",
      "24 0.5229921936988831\n",
      "25 0.4731002449989319\n",
      "26 0.4303135573863983\n",
      "27 0.39344149827957153\n",
      "28 0.3614991307258606\n",
      "29 0.33367666602134705\n",
      "30 0.3093107044696808\n",
      "31 0.28785765171051025\n",
      "32 0.26887133717536926\n",
      "33 0.251984566450119\n",
      "34 0.23689404129981995\n",
      "35 0.22334808111190796\n",
      "36 0.21113687753677368\n",
      "37 0.2000848650932312\n",
      "38 0.19004417955875397\n",
      "39 0.18088994920253754\n",
      "40 0.17251580953598022\n",
      "41 0.16483114659786224\n",
      "42 0.15775837004184723\n",
      "43 0.15123048424720764\n",
      "44 0.14518964290618896\n",
      "45 0.139585480093956\n",
      "46 0.13437438011169434\n",
      "47 0.1295178085565567\n",
      "48 0.12498217821121216\n",
      "49 0.1207379549741745\n",
      "50 0.11675868183374405\n",
      "51 0.11302132159471512\n",
      "52 0.10950502008199692\n",
      "53 0.10619150847196579\n",
      "54 0.10306409746408463\n",
      "55 0.10010809451341629\n",
      "56 0.09731017798185349\n",
      "57 0.09465838223695755\n",
      "58 0.09214175492525101\n",
      "59 0.08975056558847427\n",
      "60 0.08747592568397522\n",
      "61 0.08530964702367783\n",
      "62 0.08324437588453293\n",
      "63 0.08127342164516449\n",
      "64 0.07939061522483826\n",
      "65 0.07759024202823639\n",
      "66 0.0758671835064888\n",
      "67 0.07421661168336868\n",
      "68 0.07263419777154922\n",
      "69 0.07111591845750809\n",
      "70 0.06965795904397964\n",
      "71 0.06825686991214752\n",
      "72 0.06690949201583862\n",
      "73 0.0656128078699112\n",
      "74 0.06436407566070557\n",
      "75 0.06316083669662476\n",
      "76 0.062000520527362823\n",
      "77 0.06088101118803024\n",
      "78 0.05980021134018898\n",
      "79 0.058756232261657715\n",
      "80 0.05774718523025513\n",
      "81 0.056771330535411835\n",
      "82 0.05582718551158905\n",
      "83 0.054913271218538284\n",
      "84 0.054028064012527466\n",
      "85 0.053170353174209595\n",
      "86 0.05233881622552872\n",
      "87 0.0515323169529438\n",
      "88 0.05074986070394516\n",
      "89 0.04999019578099251\n",
      "90 0.04925258085131645\n",
      "91 0.04853590950369835\n",
      "92 0.04783938452601433\n",
      "93 0.047162216156721115\n",
      "94 0.046503547579050064\n",
      "95 0.045862723141908646\n",
      "96 0.04523896053433418\n",
      "97 0.044631604105234146\n",
      "98 0.04404006525874138\n",
      "99 0.04346369206905365\n",
      "100 0.042901959270238876\n",
      "101 0.0423542857170105\n",
      "102 0.041820209473371506\n",
      "103 0.04129915311932564\n",
      "104 0.040790725499391556\n",
      "105 0.04029446840286255\n",
      "106 0.0398099347949028\n",
      "107 0.03933672979474068\n",
      "108 0.03887446969747543\n",
      "109 0.03842276707291603\n",
      "110 0.03798123821616173\n",
      "111 0.037549618631601334\n",
      "112 0.037127524614334106\n",
      "113 0.03671469911932945\n",
      "114 0.03631076216697693\n",
      "115 0.03591550886631012\n",
      "116 0.03552869334816933\n",
      "117 0.03514992445707321\n",
      "118 0.034779079258441925\n",
      "119 0.0344158336520195\n",
      "120 0.03405999764800072\n",
      "121 0.03371131420135498\n",
      "122 0.03336965665221214\n",
      "123 0.033034708350896835\n",
      "124 0.03270633891224861\n",
      "125 0.03238435462117195\n",
      "126 0.03206856921315193\n",
      "127 0.03175878897309303\n",
      "128 0.03145488724112511\n",
      "129 0.03115661069750786\n",
      "130 0.030863896012306213\n",
      "131 0.030576612800359726\n",
      "132 0.0302945114672184\n",
      "133 0.030017463490366936\n",
      "134 0.029745405539870262\n",
      "135 0.02947821095585823\n",
      "136 0.02921568974852562\n",
      "137 0.02895771525800228\n",
      "138 0.028704287484288216\n",
      "139 0.028455153107643127\n",
      "140 0.028210189193487167\n",
      "141 0.02796945348381996\n",
      "142 0.027732698246836662\n",
      "143 0.027499858289957047\n",
      "144 0.02727087028324604\n",
      "145 0.02704567089676857\n",
      "146 0.026824072003364563\n",
      "147 0.026606012135744095\n",
      "148 0.026391487568616867\n",
      "149 0.026180313900113106\n",
      "150 0.025972425937652588\n",
      "151 0.025767823681235313\n",
      "152 0.025566380470991135\n",
      "153 0.025368036702275276\n",
      "154 0.025172725319862366\n",
      "155 0.024980325251817703\n",
      "156 0.02479083463549614\n",
      "157 0.02460418827831745\n",
      "158 0.02442026510834694\n",
      "159 0.024239063262939453\n",
      "160 0.02406051941215992\n",
      "161 0.02388450875878334\n",
      "162 0.02371109463274479\n",
      "163 0.023540088906884193\n",
      "164 0.023371554911136627\n",
      "165 0.023205365985631943\n",
      "166 0.02304152213037014\n",
      "167 0.022879961878061295\n",
      "168 0.02272062376141548\n",
      "169 0.022563444450497627\n",
      "170 0.022408422082662582\n",
      "171 0.02225549705326557\n",
      "172 0.02210460603237152\n",
      "173 0.02195574901998043\n",
      "174 0.021808862686157227\n",
      "175 0.02166394703090191\n",
      "176 0.02152087911963463\n",
      "177 0.021379658952355385\n",
      "178 0.02124028652906418\n",
      "179 0.021102696657180786\n",
      "180 0.020966893061995506\n",
      "181 0.02083274908363819\n",
      "182 0.020700328052043915\n",
      "183 0.020569628104567528\n",
      "184 0.020440464839339256\n",
      "185 0.020312901586294174\n",
      "186 0.020186934620141983\n",
      "187 0.020062504336237907\n",
      "188 0.019939610734581947\n",
      "189 0.01981819048523903\n",
      "190 0.019698243588209152\n",
      "191 0.019579648971557617\n",
      "192 0.019462525844573975\n",
      "193 0.01934681460261345\n",
      "194 0.01923239417374134\n",
      "195 0.019119322299957275\n",
      "196 0.019007598981261253\n",
      "197 0.018897103145718575\n",
      "198 0.018787896260619164\n",
      "199 0.01867997646331787\n",
      "200 0.018573220819234848\n",
      "201 0.018467752262949944\n",
      "202 0.018363388255238533\n",
      "203 0.018260186538100243\n",
      "204 0.01815815083682537\n",
      "205 0.01805727742612362\n",
      "206 0.017957447096705437\n",
      "207 0.017858777195215225\n",
      "208 0.017761090770363808\n",
      "209 0.017664503306150436\n",
      "210 0.017568957060575485\n",
      "211 0.017474452033638954\n",
      "212 0.01738092675805092\n",
      "213 0.01728837937116623\n",
      "214 0.017196809872984886\n",
      "215 0.01710622012615204\n",
      "216 0.017016548663377762\n",
      "217 0.01692785508930683\n",
      "218 0.016840016469359398\n",
      "219 0.016753096133470535\n",
      "220 0.01666703075170517\n",
      "221 0.016581881791353226\n",
      "222 0.016497589647769928\n",
      "223 0.016414152458310127\n",
      "224 0.0163315087556839\n",
      "225 0.016249721869826317\n",
      "226 0.016168728470802307\n",
      "227 0.01608852855861187\n",
      "228 0.016009122133255005\n",
      "229 0.01593051105737686\n",
      "230 0.015852630138397217\n",
      "231 0.015775544568896294\n",
      "232 0.015699191018939018\n",
      "233 0.01562350895255804\n",
      "234 0.015548620373010635\n",
      "235 0.015474402345716953\n",
      "236 0.015400855801999569\n",
      "237 0.015328041277825832\n",
      "238 0.015255958773195744\n",
      "239 0.015184485353529453\n",
      "240 0.01511368341743946\n",
      "241 0.015043491497635841\n",
      "242 0.014974030666053295\n",
      "243 0.014905119314789772\n",
      "244 0.014836878515779972\n",
      "245 0.01476924680173397\n",
      "246 0.014702224172651768\n",
      "247 0.014635810628533363\n",
      "248 0.014570007100701332\n",
      "249 0.01450475212186575\n",
      "250 0.014440106227993965\n",
      "251 0.014376007951796055\n",
      "252 0.014312518760561943\n",
      "253 0.01424951758235693\n",
      "254 0.01418712455779314\n",
      "255 0.01412521954625845\n",
      "256 0.014063922688364983\n",
      "257 0.01400305237621069\n",
      "258 0.01394279021769762\n",
      "259 0.013883015140891075\n",
      "260 0.013823727145791054\n",
      "261 0.013764927163720131\n",
      "262 0.013706672936677933\n",
      "263 0.013648906722664833\n",
      "264 0.013591566123068333\n",
      "265 0.013534711673855782\n",
      "266 0.013478344306349754\n",
      "267 0.013422464020550251\n",
      "268 0.013367068953812122\n",
      "269 0.013312039896845818\n",
      "270 0.013257496990263462\n",
      "271 0.01320344116538763\n",
      "272 0.013149749487638474\n",
      "273 0.013096543960273266\n",
      "274 0.013043764047324657\n",
      "275 0.012991349212825298\n",
      "276 0.012939420528709888\n",
      "277 0.012887856923043728\n",
      "278 0.012836718000471592\n",
      "279 0.012786004692316055\n",
      "280 0.012735655531287193\n",
      "281 0.01268573198467493\n",
      "282 0.012636173516511917\n",
      "283 0.012586979195475578\n",
      "284 0.012538149021565914\n",
      "285 0.01248974446207285\n",
      "286 0.012441704049706459\n",
      "287 0.012394027784466743\n",
      "288 0.012346716597676277\n",
      "289 0.012299708090722561\n",
      "290 0.012253125198185444\n",
      "291 0.012206845916807652\n",
      "292 0.012160930782556534\n",
      "293 0.01211537979543209\n",
      "294 0.012070132419466972\n",
      "295 0.012025249190628529\n",
      "296 0.01198066957294941\n",
      "297 0.01193645317107439\n",
      "298 0.01189254131168127\n",
      "299 0.0118489321321249\n",
      "300 0.01180562749505043\n"
     ]
    }
   ],
   "source": [
    "epochs=300\n",
    "lr=0.01\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    z = x * w + b\n",
    "    y_pred = torch.sigmoid(z)\n",
    "    loss = binary_cross_entropy_loss(y,y_pred)\n",
    "    loss.backward()    \n",
    "    print(epoch, loss.item())\n",
    "\n",
    "    with torch.no_grad():  # Perform manual gradient descent\n",
    "        w -= lr * w.grad\n",
    "        b -= lr * b.grad\n",
    "\n",
    "    w.grad.zero_()  # Reset gradients\n",
    "    b.grad.zero_()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0117, grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = w*x+b\n",
    "y_pred = torch.sigmoid(z)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleANN, self).__init__()\n",
    "        \n",
    "        # Input size = 784 (for MNIST: 28x28 images flattened)\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)  # 10 classes for classification\n",
    "\n",
    "        # Optional dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)          # Flatten image to 784-dim vector\n",
    "        x = F.relu(self.fc1(x))      # Hidden layer 1\n",
    "        x = self.dropout(x)          # Dropout for regularization\n",
    "        x = F.relu(self.fc2(x))      # Hidden layer 2\n",
    "        x = self.fc3(x)              # Output layer (logits)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(nn.Module):\n",
    "    def init(super):\n",
    "        self.conv1 = nn.conv2d(4,1,padding = 3,strider = 1)    \n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.fc = nn.Linear(2*12)\n",
    "        return(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
